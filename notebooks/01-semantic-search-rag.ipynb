{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Semantic search based RAG\n",
    "\n",
    "We are going to use LlamaIndex to build a basic RAG pipeline that will use one of the open source embedding models. Then, we will consider different optimizations to either improve the performance or reduce the cost of the pipeline.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe3d1bd3cb7874cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the configuration\n",
    "\n",
    "Before we start, all the configuration is loaded from the `.env` file we created in the previous notebook."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23e281aaf592e306"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic RAG setup\n",
    "\n",
    "We will be using one of the open source embedding models to vectorize our document (actually, the snapshots we imported in the previous notebook were generated using the same model, so we need to use it for queries as well). OpenAI GPT will be our LLM, and it is the default model for LlamaIndex, so there is no need to configure it explicitly.\n",
    "\n",
    "The vector index, which will act as a fast retrieval layer, is the last missing piece to build our basic semantic search RAG. Qdrant will serve that purpose, as all the documents are already there."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45e4dd797cbe142a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=\"local:BAAI/bge-large-en\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ceecec79071db759",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "import os\n",
    "\n",
    "client = QdrantClient(\n",
    "    os.environ.get(\"QDRANT_URL\"), \n",
    "    api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    ")\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client, \n",
    "    collection_name=\"hacker-news\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef2c13ee4a83a508",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    service_context=service_context,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dca3d121903e207",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Querying RAG\n",
    "\n",
    "LlamaIndex simplifies the querying process by providing a high-level API that abstracts the underlying complexity. We can use the `as_query_engine` method to create a query engine that will handle the entire process for us, with the default configuration."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1f73a5b0023dae8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the best way to learn programming?\")\n",
    "print(response.response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b10b5aec9a9aeb1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our RAG retrieves some possibly relevant documents by using the original prompt as a query, and then sends them as a part of the prompt to the LLM. It seems to be a good idea to check what were these documents, and if our LLM was not making up the answer using its internal knowledge."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7ce6484a8eaac59"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i, node in enumerate(response.source_nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a62a2fb53edaa8c1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first tweak we can consider is to increase the number of documents fetched from our knowledge base (the default of LlamaIndex is just 2). We can do that by setting the `similarity_top_k` parameter of the `as_query_engine` method."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a392c560d622699d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = index \\\n",
    "    .as_query_engine(similarity_top_k=5) \\\n",
    "    .query(\"What is the best way to learn programming?\")\n",
    "print(response.response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e456cefe13dde93c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i, node in enumerate(response.source_nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c72f6f232be4b89",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Customizing the RAG pipeline\n",
    "\n",
    "The defaults of LlamaIndex are a good starting point, but we can customize the pipeline to better fit our needs. That gives us more control over the behavior of the semantic search retriever or the way we interact with the LLM. LlamaIndex has pretty decent support for customizing the pipeline and there are three components that we need to set up:\n",
    "\n",
    "1. Retriever\n",
    "2. Response synthesizer\n",
    "3. Query engine"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a59904fb28cfe369"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.indices.vector_store import VectorIndexRetriever\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b73b609cef8b1916",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is the best way to learn programming?\")\n",
    "print(response.response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59ff323f5dc019ed",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Playing with response synthesizers\n",
    "\n",
    "Response synthesizers are responsible for interactions with the LLM. This a component we want to control, when it comes to prompts and the way we actually communicate with the language model. There are lots of parameters to tweak, and prompt engineering is a topic of its own. Thus, we won't play with it too, but we can at least test out different response modes.\n",
    "\n",
    "The default one is `ResponseMode.COMPACT`, that combines retrieved text chunks into larger pieces, to utilize the available context window. There are also plenty of other modes, and they may work best in some specific scenario. For example, some of the modes may make a separate LLM call per extracted text chunk, which may be beneficial in some cases, but also increase the cost of the pipeline.\n",
    "\n",
    "Let's just compare the previous response with the `ResponseMode.ACCUMULATE` and `ResponseMode.REFINE` modes. The first one should create a response for each chunk and the concatenate them, while the second one should make a separate LLM call for each chunk in an iterative manner. That means, each call will use the previous response as a context."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98b489a71360ffa2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index.response_synthesizers import ResponseMode\n",
    "\n",
    "accumulate_response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=ResponseMode.ACCUMULATE,\n",
    ")\n",
    "\n",
    "accumulate_query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=accumulate_response_synthesizer,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb47addbdc849fcf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = accumulate_query_engine.query(\"What is the best way to learn programming?\")\n",
    "print(response.response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d895eaf9f042fd22",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "refine_response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=ResponseMode.REFINE,\n",
    ")\n",
    "\n",
    "refine_query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=refine_response_synthesizer,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c82c017459bfb664",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = refine_query_engine.query(\"What is the best way to learn programming?\")\n",
    "print(response.response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e748b91db81b9336",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multitenancy\n",
    "\n",
    "Most of the real applications require some sort of data separation. If you collect data coming from different users or organizations, you probably don't want to mix them up in the answers. Quite a common mistake, while using Qdrant, is to create a separate collection for each tenant. Instead, you can use the metadata field to separate the data. This field should have a payload index created, so the operations are fast. \n",
    "\n",
    "This is a Qdrant-specific feature, and the configuration is not done in LlamaIndex, but in Qdrant itself. However, we passed an instance of `QdrantClient` to the `QdrantVectorStore`, so we can use it to create a payload index for the metadata field.\n",
    "\n",
    "In our case, we can consider splitting the data by the type of the document. We have two types of documents in our collection: `story` and `comment`. We can use the `type` field to separate them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "235e641164884b02"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "client.create_payload_index(\n",
    "    collection_name=\"hacker-news\",\n",
    "    field_name=\"type\",\n",
    "    field_schema=models.PayloadSchemaType.KEYWORD,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb261d6fa91e727",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the newly created payload index, we can filter the documents by type. That's why we wanted to customize the pipeline, so we can add this filter to the retriever."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5077c72c40add0d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import MetadataFilters, MetadataFilter\n",
    "\n",
    "filtering_retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=5,\n",
    "    filters=MetadataFilters(\n",
    "        filters=[\n",
    "            MetadataFilter(key=\"type\", value=\"story\"),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "filtering_query_engine = RetrieverQueryEngine(\n",
    "    retriever=filtering_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "448493cd6c4d1c32",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = filtering_query_engine.query(\"What is the best way to learn programming?\")\n",
    "print(response.response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4e00178a2746d09",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i, node in enumerate(response.source_nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f05c4c83cf54647",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Additional tweaks\n",
    "\n",
    "Some scenarios require different means than just semantic search. For example, if we want to prefer the most recent documents, none of the embedding models is going to capture it, since it is a cross-document relationship. LlamaIndex provides a way to add additional postprocessing, so we can include the additional constraints directly on the prefetched documents.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88f35208132ff94e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index.postprocessor import FixedRecencyPostprocessor\n",
    "\n",
    "prefetching_retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=25,  # prefetch way more documents\n",
    "    filters=MetadataFilters(\n",
    "        filters=[\n",
    "            MetadataFilter(key=\"type\", value=\"comment\"),  # we want comments this time\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "recency_query_engine = RetrieverQueryEngine(\n",
    "    retriever=prefetching_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[\n",
    "        FixedRecencyPostprocessor(\n",
    "            service_context=service_context,\n",
    "            date_key=\"date\",  # date is the default key also, but make it explicit\n",
    "            top_k=5,  # leave just 20% of the prefetched documents\n",
    "        )\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "924db74f9afcb13f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = recency_query_engine.query(\"What is the best way to learn programming?\")\n",
    "print(response.response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36f4611d9c0d262a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i, node in enumerate(response.source_nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c074064ccbfc715",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index.postprocessor import EmbeddingRecencyPostprocessor\n",
    "\n",
    "embedding_recency_query_engine = RetrieverQueryEngine(\n",
    "    retriever=prefetching_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[\n",
    "        EmbeddingRecencyPostprocessor(\n",
    "            service_context=service_context,\n",
    "            date_key=\"date\",  # date is the default key\n",
    "            similarity_cutoff=0.9,\n",
    "        )\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf76ad82f0d270bb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = embedding_recency_query_engine.query(\"What is the best way to learn programming?\")\n",
    "print(response.response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33ea0ba3f3d10b13",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i, node in enumerate(response.source_nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cbe86011e834e79",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "620de1ff4231ef30",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
